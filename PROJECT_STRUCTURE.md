# Project Structure

This document outlines the recommended directory structure for the Roaree Benchmark Club repository.

## ğŸ“ Directory Organization

```
roareebenchmark/
â”œâ”€â”€ .github/                    # GitHub-specific files
â”‚   â”œâ”€â”€ ISSUE_TEMPLATE/        # Issue templates
â”‚   â””â”€â”€ pull_request_template.md
â”œâ”€â”€ benchmarks/                 # Benchmark implementations
â”‚   â”œâ”€â”€ benchmark_name/
â”‚   â”‚   â”œâ”€â”€ README.md          # Benchmark documentation
â”‚   â”‚   â”œâ”€â”€ data/              # Benchmark datasets
â”‚   â”‚   â”œâ”€â”€ scripts/           # Evaluation scripts
â”‚   â”‚   â””â”€â”€ results/           # Benchmark results
â”‚   â””â”€â”€ ...
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ evaluation/            # Evaluation frameworks
â”‚   â”œâ”€â”€ metrics/               # Metric implementations
â”‚   â”œâ”€â”€ models/                # Model wrappers and interfaces
â”‚   â””â”€â”€ utils/                 # Utility functions
â”œâ”€â”€ tests/                     # Unit and integration tests
â”‚   â”œâ”€â”€ test_benchmarks/
â”‚   â”œâ”€â”€ test_evaluation/
â”‚   â””â”€â”€ test_utils/
â”œâ”€â”€ notebooks/                 # Jupyter notebooks for analysis
â”‚   â”œâ”€â”€ exploratory/           # Exploratory data analysis
â”‚   â””â”€â”€ results/               # Results visualization
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ guides/                # How-to guides
â”‚   â”œâ”€â”€ methodology/           # Methodology documentation
â”‚   â””â”€â”€ api/                   # API documentation
â”œâ”€â”€ data/                      # Data directory (gitignored)
â”‚   â”œâ”€â”€ raw/                   # Raw datasets
â”‚   â””â”€â”€ processed/             # Processed datasets
â”œâ”€â”€ models/                    # Model storage (gitignored)
â”‚   â”œâ”€â”€ checkpoints/           # Model checkpoints
â”‚   â””â”€â”€ pretrained/            # Pretrained models
â”œâ”€â”€ results/                   # Results directory (gitignored)
â”‚   â”œâ”€â”€ experiments/           # Experiment results
â”‚   â””â”€â”€ figures/               # Generated figures
â”œâ”€â”€ configs/                   # Configuration files
â”‚   â””â”€â”€ experiment_configs/    # Experiment configurations
â”œâ”€â”€ scripts/                   # Utility scripts
â”‚   â”œâ”€â”€ setup/                 # Setup scripts
â”‚   â””â”€â”€ analysis/              # Analysis scripts
â”œâ”€â”€ .gitignore                 # Git ignore rules
â”œâ”€â”€ CODE_OF_CONDUCT.md         # Code of conduct
â”œâ”€â”€ CONTRIBUTING.md            # Contribution guidelines
â”œâ”€â”€ CONTRIBUTORS.md            # List of contributors
â”œâ”€â”€ LICENSE                    # License information
â”œâ”€â”€ README.md                  # Main readme
â”œâ”€â”€ SECURITY.md                # Security policy
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ setup.py                   # Package setup
â””â”€â”€ pyproject.toml            # Modern Python project config
```

## ğŸ“‚ Directory Purposes

### `/benchmarks/`
Contains individual benchmark implementations. Each benchmark should have its own subdirectory with:
- `README.md`: Description, methodology, and usage
- `data/`: Dataset or data loading scripts
- `scripts/`: Evaluation and analysis scripts
- `results/`: Benchmark results and leaderboards

### `/src/`
Core source code for the project:
- `evaluation/`: Framework for running evaluations
- `metrics/`: Custom metric implementations
- `models/`: Model wrappers for different LLM APIs
- `utils/`: Shared utility functions

### `/tests/`
Test suite following the structure of `/src/`:
- Unit tests for all source modules
- Integration tests for benchmarks
- Test fixtures and utilities

### `/notebooks/`
Jupyter notebooks for analysis and visualization:
- `exploratory/`: Data exploration and initial analyses
- `results/`: Results visualization and reporting

### `/docs/`
Project documentation:
- `guides/`: Tutorials and how-to guides
- `methodology/`: Detailed methodology documentation
- `api/`: API reference documentation

### `/data/` (gitignored)
Data storage (not committed to repository):
- `raw/`: Original datasets
- `processed/`: Cleaned and processed datasets
- Store large datasets externally (e.g., Google Drive, S3)

### `/models/` (gitignored)
Model storage (not committed to repository):
- `checkpoints/`: Model checkpoints
- `pretrained/`: Downloaded pretrained models
- Store large models externally

### `/results/` (gitignored)
Experiment results (not committed to repository):
- `experiments/`: Individual experiment outputs
- `figures/`: Generated visualizations
- Summary results can be committed in `/benchmarks/*/results/`

### `/configs/`
Configuration files:
- Experiment configurations (YAML/JSON)
- Model configurations
- Evaluation settings

### `/scripts/`
Standalone scripts:
- `setup/`: Environment setup and installation
- `analysis/`: Data analysis and processing

## ğŸ“ File Naming Conventions

### Python Files
- Use `snake_case` for file names
- Test files: `test_*.py`
- Use descriptive names: `evaluate_benchmark.py` not `eval.py`

### Notebooks
- Use descriptive names with dates: `2025-11-11_initial_analysis.ipynb`
- Prefix with number for ordering: `01_data_exploration.ipynb`

### Configuration Files
- Use clear, descriptive names
- Include purpose: `bert_evaluation_config.yaml`

### Data Files
- Include version or date: `dataset_v1.csv`
- Document data sources in README

## ğŸ”§ Configuration Files

### `requirements.txt`
Pin major versions, allow minor updates:
```
torch>=2.0.0,<3.0.0
transformers>=4.30.0,<5.0.0
```

### `pyproject.toml`
Modern Python project configuration (recommended):
- Package metadata
- Build system requirements
- Tool configurations (black, pytest, mypy)

### `setup.py`
For package installation and distribution

## ğŸ“Š Data Management

### Large Files
- **Do not commit** files larger than 10MB
- Use Git LFS for files between 10MB-100MB
- Store files >100MB externally
- Document external storage locations

### Datasets
- Keep raw data immutable
- Document preprocessing steps
- Version processed datasets
- Include data licenses and citations

## ğŸ§ª Testing Organization

- Mirror the structure of `/src/` in `/tests/`
- One test file per source file
- Use fixtures for common setup
- Include integration tests for benchmarks

## ğŸ“š Documentation Standards

### Code Documentation
- Docstrings for all public functions/classes
- Type hints for function signatures
- Inline comments for complex logic

### README Files
- Every major directory should have a README
- Include purpose, usage, and examples
- Link to related documentation

## ğŸ”„ Version Control

### Branches
- `main`: Stable, production-ready code
- `develop`: Integration branch for features
- `feature/*`: Individual feature branches
- `hotfix/*`: Critical bug fixes

### Commits
- Use clear, descriptive commit messages
- Reference issues: `Fixes #123`
- Group related changes

## ğŸš€ Getting Started

New members should:
1. Clone the repository
2. Create a virtual environment
3. Install dependencies: `pip install -r requirements.txt`
4. Run tests: `pytest tests/`
5. Read benchmark READMEs to understand existing work

## â“ Questions?

- Check the documentation in `/docs/`
- Ask in team discussions
- Contact core team members

---

*This structure is a recommendation and may evolve as the project grows.*

*Roar, Lions, Roar! ğŸ¦*
